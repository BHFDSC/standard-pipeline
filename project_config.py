{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61f276ca-d89f-4f13-a047-5aa4e97c8e97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Project Configuration\n",
    "# ⚠️ Please do not edit this file directly ⚠️\n",
    "\n",
    "# This notebook sets up project paths, environment variables, and configuration settings automatically. It will be loaded in each subsequent notebook to ensure consistency across the project by setting up the environment and loading shared resources. Manual edits may cause errors or break dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f77480a-76e7-4fb6-b2e6-dd3e456e7407",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Project directory\n",
    "project_directory = os.path.basename(os.path.dirname(dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()))\n",
    "\n",
    "# Parent dictory\n",
    "parent_directory = (f'/Workspace{os.path.dirname(os.path.dirname(dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()) )}')\n",
    "\n",
    "# Username\n",
    "username = spark.sql('select current_user() as user').collect()[0]['user'] #dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply('user')\n",
    "\n",
    "# Set working directory to parent to read in functions\n",
    "os.chdir(f'{parent_directory}')\n",
    "project_name = f'{project_directory}'\n",
    "project_folder = f'{parent_directory}/{project_name}/'\n",
    "\n",
    "# Assign to environment \n",
    "os.environ['USERNAME'] = username\n",
    "os.environ['PROJECT_FOLDER'] = project_folder\n",
    "os.environ['PROJECT_NAME'] = project_name\n",
    "\n",
    "# Disable cache\n",
    "spark.conf.set(\"spark.databricks.io.cache.enabled\", \"false\")\n",
    "\n",
    "# Database names\n",
    "db = 'dars_nic_391419_j3w9t'\n",
    "dbc = f'{db}_collab'\n",
    "dsa = 'dsa_391419_j3w9t_collab'\n",
    "dss = 'dss_corporate'\n",
    "\n",
    "\n",
    "# Copy current table_mapping from table_management\n",
    "hds_central = f'/Workspace/Shared/hds_team'\n",
    "hds_central_management = f'{hds_central}/hds_table_management'\n",
    "hds_central_monitoring = f'{hds_central}/hds_table_monitoring'\n",
    "table_mapping_source = f'{hds_central_management}/config/table_mapping.json'\n",
    "table_mapping_destination = f'{parent_directory}/{project_directory}/config/table_mapping.json'\n",
    "shutil.copyfile(table_mapping_source, table_mapping_destination);\n",
    "table_mapping_path = f'{parent_directory}/{project_directory}/config/table_mapping.json'\n",
    "\n",
    "archived_base_path = f'{hds_central_monitoring}/outputs'\n",
    "archive_folders = [f for f in os.listdir(archived_base_path) if os.path.isdir(os.path.join(archived_base_path, f))]"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "project_config",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
